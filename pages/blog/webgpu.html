<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="darkreader-lock">
    <link rel="stylesheet" href="../../assets/libraries/bootstrap5.0.2/bootstrap.min.css">
    <link rel="stylesheet" href="../../assets/libraries/fontawesome6.5.1/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
    <script src="https://public.flourish.studio/resources/embed.js"></script>
    
    <link rel="stylesheet" href="../../assets/css/style.css">
    <link rel="stylesheet" href="../../assets/css/github-markdown-dark.css">
    <script src="../../assets/js/common.js"></script>
    <title>WebGPU</title>
	
	<link rel="apple-touch-icon" sizes="180x180" href="../../assets/img/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../assets/img/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../../assets/img/favicon/favicon-16x16.png">
<link rel="manifest" href="../../assets/img/favicon/site.webmanifest">
<link rel="mask-icon" href="../../assets/img/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../../assets/img/favicon/favicon.ico">
<meta name="msapplication-TileColor" content="#941739">
<meta name="msapplication-config" content="../../assets/img/favicon/browserconfig.xml">
<meta name="theme-color" content="#ffffff">
</head>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-5RWJF4E3GR"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-5RWJF4E3GR');
</script>

<body>
<div id="HTMLdata"
section="blog">
</div>
<div style="display: flex; flex-direction: column; height: 100vh; width: 100vw; overflow-x: hidden; background-color: rgb(35, 39, 50);">
	<div id="HTMLtopbar" style="flex: 0;"><div class="row" style="display: flex;" id="linksTopbar">
    <div style="flex: 6; display: flex; align-items: center; background-color: rgb(14, 60, 76); color: white; height: 100%;">
        &emsp;<a href="../../pages/about.html" style="text-decoration: none; color: white;" title="Who am I?">Arbelo Cabrera, Pablo</a>
        <!-- &emsp;<a class="linkPopUp" href="https://www.youtube.com/@pactomars"><i class="fa-brands fa-youtube fa-lg" title="YouTube channel" style="color: rgb(255, 129, 129)"></i></a>
        &emsp;<a class="linkPopUp" href="https://www.flickr.com/photos/200282744@N03/albums" title="flickr albums"><img height="45em" src="../../assets/img/icons/flickr.svg" alt="flickr official logo."></a> -->
        &emsp;<a class="linkPopUp" href="https://www.linkedin.com/in/pablo-arbelo-cabrera-051a951a2"><i class="fa-brands fa-linkedin-in fa-lg" title="LinkedIn profile" style="color: rgb(90, 176, 247)"></i></a>
        &emsp;<a class="linkPopUp" href="https://github.com/sp-droid"><i class="fa-brands fa-github fa-lg" title="GitHub profile" style="color: rgb(230, 237, 243)"></i></a>
    </div>
    <div style="flex: 1; display: flex; align-items: center; justify-content: center; background-color: rgb(76, 57, 2); height: 100%;">
        <a class="linkPopUp" title="For work or other topics" href="https://docs.google.com/forms/d/1pK5hPSywf1resOSLIUOESFpqQn6cnRa2YjkRF7d77D8" style="font-weight: 500; color: rgb(222, 237, 240)">Contact</a>
    </div>
</div>
<div class="row" id="navTopbar" style="background: linear-gradient(to right, rgb(48, 40, 2), rgb(35, 38, 39))">
    <div class="col d-flex align-items-center justify-content-center" style="line-height: 20px; height: 100%;">
        <!-- <a id="indexMenuButton" href="../../pages/index.html" title="Homepage where recent or current projects/entries/news are showcased">Home</a> -->
        <a id="projectsMenuButton" href="../../pages/projects.html" title="Projects of different kinds are highlighted, sometimes as a video, as a JS browser-app or as an entire webpage">Projects</a>
        <a id="recipesMenuButton" href="../../pages/recipes.html" title="Some of my recipes since I like cooking quite a bit">Recipes</a>
        <a id="blogMenuButton" href="../../pages/blog.html" title="Devlogs and personal notes">Blog</a>
        <a id="aboutMenuButton" href="../../pages/about.html" title="Brief page about myself for anyone interested">About me</a>
    </div>
</div></div>
    <div style="flex: 1; display: flex;">
        <div class="markdown-body sidebarHide" style="flex: 1; min-width: 20px;"><br><div class='TOCpost'><h4>&emsp;&nbsp;Contents</h4><ul><li><a href='#section0'>(Top)</a></li><li onclick="toggleDetails(event)"><details><summary><a href='#section1'>The standard</a></summary><ul><li onclick="toggleDetails(event)"><details><summary><a href='#section2'>Useful documentation</a></summary><ul><li><a href='#section3'>Built-in inputs</a></li><li><a href='#section4'>Copying data back to the CPU</a></li><li><a href='#section5'>Benchmarking GPU functions</a></li><li><a href='#section6'>Common problems</a></li><li><a href='#section7'>Optimization techniques</a></li></ul></details></li><li onclick="toggleDetails(event)"><details><summary><a href='#section8'>Algorithms</a></summary><ul><li><a href='#section9'>Parallel reduction</a></li><li><a href='#section10'>PseudoRandom Number generator</a></li></ul></details></li></ul></details></li>
</ul></div></div>
        <div class="markdown-body" id="HTMLpost"><div id="section0"></div>
<h6>March 13th, 2025</h6>
<h1>WebGPU</h1>
<h5>Reading time: 19 mins</h5>
<hr>
<p>I'm writing this article to serve me as a repository for information on WebGPU as I'm learning it. It covers useful links, architecture description and some non-flashy -but still very important- algorithms implemented on it.</p>
<p><div class='TOCpost'><h4>&emsp;&nbsp;Contents</h4><ul><li><a href='#section0'>(Top)</a></li><li onclick="toggleDetails(event)"><details><summary><a href='#section1'>The standard</a></summary><ul><li onclick="toggleDetails(event)"><details><summary><a href='#section2'>Useful documentation</a></summary><ul><li><a href='#section3'>Built-in inputs</a></li><li><a href='#section4'>Copying data back to the CPU</a></li><li><a href='#section5'>Benchmarking GPU functions</a></li><li><a href='#section6'>Common problems</a></li><li><a href='#section7'>Optimization techniques</a></li></ul></details></li><li onclick="toggleDetails(event)"><details><summary><a href='#section8'>Algorithms</a></summary><ul><li><a href='#section9'>Parallel reduction</a></li><li><a href='#section10'>PseudoRandom Number generator</a></li></ul></details></li></ul></details></li>
</ul></div></p>
<div id="section1"></div>
<h1>The standard</h1>
<p>WebGPU is a graphics <abbr title="Application Programming Interface">API</abbr> standard being developed over the last few years, built as a performant method to bridge graphics programming between different platforms:</p>
<ul>
<li><strong>Metal</strong>, an <abbr title="Application Programming Interface">API</abbr> for Apple products. I'm mentioning it first because apparently it's the one WebGPU is most similar to.</li>
<li><strong>Vulkan</strong>, open source and very low level.</li>
<li><strong>DirectX</strong>, for Microsoft products.</li>
<li><strong>OpenGL</strong>, open source no compute <abbr title="Application Programming Interface">API</abbr>, it has been showing its age for some time.</li>
<li><strong>WebGL</strong>, a port of an OpenGL version for the web. However, it didn't really stick and soon suffered from the same issue.</li>
<li><strong>CUDA</strong>, compute <abbr title="Application Programming Interface">API</abbr> for NVIDIA GPUs</li>
<li><strong>ROCm</strong>, compute <abbr title="Application Programming Interface">API</abbr> for AMD GPUs</li>
<li><strong>OpenCL</strong>, open source compute <abbr title="Application Programming Interface">API</abbr></li>
</ul>
<p>As you can see, there were a myriad of solutions, each with its own syntax and shader language, for each operating system, GPU or even for each period of time. What if there was a single top level <abbr title="Application Programming Interface">API</abbr> that chose which one to use but always exposed the same behavior to the user? And so WebGPU was born from the joint effort between the partners behind those APIs.</p>
<p>WebGPU's shader language is called WGSL. The syntax is quite similar to Rust, in fact, even though for now highlight.js doesn't officially support WGSL I could reproduce a similar highlight by setting the language to Rust.</p>
<h4>Implementations as of 2025</h4>
<p>There are 3 different ways of tapping into the standard:</p>
<ul>
<li>For the web* in JS, through the <strong>WebGPU JS</strong> <abbr title="Application Programming Interface">API</abbr></li>
<li>For native apps in C/C++, through <strong>Dawn</strong>, developed by Google.</li>
<li>For native apps in Rust, through <strong>WGPU</strong>, developed by Mozilla.</li>
</ul>
<p>*: It's also available for native apps through some of the JS runtime environments like Node.js</p>
<p>If you are a beginner in strongly typed languages, I recommend to build a few small projects through the JS <abbr title="Application Programming Interface">API</abbr> first, so you don't need to worry about garbage collection. I find it quite easy to get into it in this way. This <a href="https://codelabs.developers.google.com/your-first-webgpu-app">tutorial</a> was especially good.</p>
<p>So, WebGPU is the GPU <abbr title="Application Programming Interface">API</abbr> with the biggest backing in history, not so low level, with access to compute shaders, performant and allows its use both for web and native apps, multiplatform in terms of OS but also in terms of the GPU vendor itself... for me it's quite hard to pass on it.</p>
<div id="section2"></div>
<h2>Useful documentation</h2>
<ul>
<li><a href="https://www.w3.org/TR/WGSL/">Official documentation</a></li>
<li><a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-limits-and-features.html">Adapter/device limits &amp; features</a></li>
</ul>
<h4>Equivalent nomenclature</h4>
<table>
<thead>
<tr>
<th>Concept</th>
<th>WebGPU (&amp; Vulkan)</th>
<th>NVIDIA</th>
<th>AMD</th>
<th>Intel</th>
<th>Apple</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Execution unit</strong></td>
<td>Invocation</td>
<td><strong>Thread</strong></td>
<td>Wavefront lane</td>
<td>EU thread</td>
<td>Thread</td>
</tr>
<tr>
<td><strong>Group of threads (SIMD capable)</strong></td>
<td>Subgroup</td>
<td><strong>Warp</strong></td>
<td>Wavefront</td>
<td>Subgroup</td>
<td>SIMD group</td>
</tr>
<tr>
<td><strong>Shared memory group</strong></td>
<td><strong>Workgroup</strong></td>
<td>Block</td>
<td>Workgroup</td>
<td>Workgroup</td>
<td>Threadgroup</td>
</tr>
<tr>
<td><strong>Task group</strong></td>
<td><strong>Dispatch</strong> of (..)</td>
<td>Grid</td>
<td>NDRange</td>
<td>Dispatch of (..)</td>
<td>Grid</td>
</tr>
</tbody>
</table>
<p>I personally like using thread, warp, workgroup and dispatch.</p>
<div id="section3"></div>
<h3>Built-in inputs</h3>
<table>
<thead>
<tr>
<th><strong>Builtin Name</strong></th>
<th><strong>Stage</strong></th>
<th>Direction</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>vertex_index</em></td>
<td>vertex</td>
<td>input</td>
<td>u32</td>
<td>Index of the current vertex within the current <abbr title="Application Programming Interface">API</abbr>-level draw command, independent of draw instancing.<br />For a non-indexed draw, the first vertex has an index equal to the <code>firstVertex</code> argument of the draw, whether provided directly or indirectly. The index is incremented by one for each additional vertex in the draw instance. For an indexed draw, the index is equal to the index buffer entry for the vertex, plus the <code>baseVertex</code> argument of the draw, whether provided directly or indirectly.</td>
</tr>
<tr>
<td><em>instance_index</em></td>
<td>vertex</td>
<td>input</td>
<td>u32</td>
<td>Instance index of the current vertex within the current <abbr title="Application Programming Interface">API</abbr>-level draw command.<br />The first instance has an index equal to the <code>firstInstance</code> argument of the draw, whether provided directly or indirectly. The index is incremented by one for each additional instance in the draw.</td>
</tr>
<tr>
<td><em>clip_distances</em></td>
<td>vertex</td>
<td>output</td>
<td>array&lt;f32,N&gt;(N&lt;=8)</td>
<td><a href="https://www.w3.org/TR/WGSL/#extension-clip_distances">Extension needed</a></td>
</tr>
<tr>
<td rowspan=2><em>position</em></td>
<td>vertex</td>
<td>output</td>
<td>vec4f</td>
<td>Output position of the current vertex, using homogeneous coordinates. After homogeneous normalization (where each of the <em>x</em>, <em>y</em>, and <em>z</em> components are divided by the <em>w</em> component), the position is in the WebGPU normalized device coordinate space. See <a href="https://www.w3.org/TR/webgpu/#coordinate-systems">WebGPU § 3.3 Coordinate Systems</a>.</td>
</tr>
<tr>

<td>fragment</td>
<td>input</td>
<td>vec4f</td>
<td>Framebuffer position of the current fragment in <a href="https://gpuweb.github.io/gpuweb/#framebuffer">framebuffer</a> space. (The <em>x</em>, <em>y</em>, and <em>z</em> components have already been scaled such that <em>w</em> is now 1.) See <a href="https://www.w3.org/TR/webgpu/#coordinate-systems">WebGPU § 3.3 Coordinate Systems</a>.</td>
</tr>
<tr>
<td><em>front_facing</em></td>
<td>fragment</td>
<td>input</td>
<td>bool</td>
<td>True when the current fragment is on a <a href="https://gpuweb.github.io/gpuweb/#front-facing">front-facing</a> primitive. False otherwise.</td>
</tr>
<tr>
<td><em>frag_depth</em></td>
<td>fragment</td>
<td>output</td>
<td>f32</td>
<td>Updated depth of the fragment, in the viewport depth range. See <a href="https://www.w3.org/TR/webgpu/#coordinate-systems">WebGPU § 3.3 Coordinate Systems</a>.</td>
</tr>
<tr>
<td><em>local_invocation_id</em></td>
<td>compute</td>
<td>input</td>
<td>vec3u</td>
<td>The current invocation’s <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html#local-invocation-id">local invocation ID</a>, i.e. its position in the <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html#workgroup-grid">workgroup grid</a>.</td>
</tr>
<tr>
<td><em>local_invocation_index</em></td>
<td>compute</td>
<td>input</td>
<td>u32</td>
<td>The current invocation’s <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html#local-invocation-index">local invocation index</a>, a linearized index of the invocation’s position within the <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html#workgroup-grid">workgroup grid</a>.</td>
</tr>
<tr>
<td><em>global_invocation_id</em></td>
<td>compute</td>
<td>input</td>
<td>vec3u</td>
<td>The current invocation’s <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html#global-invocation-id">global invocation ID</a>, i.e. its position in the <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html#compute-shader-grid">compute shader grid</a>.</td>
</tr>
<tr>
<td><em>workgroup_id</em></td>
<td>compute</td>
<td>input</td>
<td>vec3u</td>
<td>The current invocation’s <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html#workgroup-id">workgroup ID</a>, i.e. the position of the workgroup in the <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html#compute-shader-grid">compute shader grid</a>.</td>
</tr>
<tr>
<td><em>num_workgroups</em></td>
<td>compute</td>
<td>input</td>
<td>vec3u</td>
<td>The <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html#dispatch-size">dispatch size</a>, <code>vec&lt;u32&gt;(group_count_x, group_count_y, group_count_z)</code>, of the compute shader <a href="https://www.w3.org/TR/webgpu/#compute-pass-encoder-dispatch">dispatched</a> by the <abbr title="Application Programming Interface">API</abbr>.</td>
</tr>
<tr>
<td><em>subgroup_invocation_id</em></td>
<td>comp + frag</td>
<td>input</td>
<td>u32</td>
<td><a href="https://www.w3.org/TR/WGSL/#extension-subgroups">Extension needed</a></td>
</tr>
<tr>
<td><em>subgroup_size</em></td>
<td>comp + frag</td>
<td>input</td>
<td>u32</td>
<td><a href="https://www.w3.org/TR/WGSL/#extension-subgroups">Extension needed</a></td>
</tr>
<tr>
<td><em>sample_index</em></td>
<td>fragment</td>
<td>input</td>
<td>u32</td>
<td>Sample index for the current fragment. The value is least 0 and at most <code>sampleCount</code>-1, where <code>sampleCount</code> is the MSAA sample <code>count</code> specified for the GPU render pipeline.<br/>See <a href="https://www.w3.org/TR/webgpu/#gpurenderpipeline">WebGPU § 10.3 GPURenderPipeline</a>.</td>
</tr>
<tr>
<td rowspan=2><em>sample_mask</em></td>
<td>fragment</td>
<td>input</td>
<td>u32</td>
<td>Sample coverage mask for the current fragment. It contains a bitmask indicating which samples in this fragment are covered by the primitive being rendered.<br/>See <a href="https://www.w3.org/TR/webgpu/#sample-masking">WebGPU § 23.3.11 Sample Masking</a>.</td>
</tr>
<tr>

<td>fragment</td>
<td>output</td>
<td>u32</td>
<td>Sample coverage mask control for the current fragment. The last value written to this variable becomes the <a href="https://gpuweb.github.io/gpuweb/#shader-output-mask">shader-output mask</a>. Zero bits in the written value will cause corresponding samples in the color attachments to be discarded.<br/>See <a href="https://www.w3.org/TR/webgpu/#sample-masking">WebGPU § 23.3.11 Sample Masking</a>.</td>
</tr>
</tbody>
</table>
<div id="section4"></div>
<h3>Copying data back to the CPU</h3>
<p>We need:</p>
<ul>
<li>A source, a GPU buffer defined with the flag <code>GPUBufferUsage.COPY_SRC</code></li>
<li>An intermediate or staging buffer</li>
</ul>
<pre><code class="language-js"><span class="hljs-keyword">const</span> stagingBuffer = device.<span class="hljs-title function_">createBuffer</span>({
    <span class="hljs-attr">size</span>: <span class="hljs-number">4</span>,
    <span class="hljs-attr">usage</span>: <span class="hljs-title class_">GPUBufferUsage</span>.<span class="hljs-property">COPY_DST</span> | <span class="hljs-title class_">GPUBufferUsage</span>.<span class="hljs-property">MAP_READ</span>
});
</code></pre>
<ul>
<li>Define copy and synchronize devices, just before submitting the instructions</li>
</ul>
<pre><code class="language-js">encoder.<span class="hljs-title function_">copyBufferToBuffer</span>(sourceBuffer, <span class="hljs-number">0</span>, stagingBuffer, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>);
device.<span class="hljs-property">queue</span>.<span class="hljs-title function_">submit</span>([encoder.<span class="hljs-title function_">finish</span>()]);

<span class="hljs-comment">// Wait for GPU work to finish and map the buffer for CPU access</span>
<span class="hljs-keyword">await</span> stagingBuffer.<span class="hljs-title function_">mapAsync</span>(<span class="hljs-title class_">GPUMapMode</span>.<span class="hljs-property">READ</span>);
<span class="hljs-keyword">const</span> data = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Uint32Array</span>(buffer.<span class="hljs-title function_">getMappedRange</span>());
<span class="hljs-comment">// console.log(data[0]);</span>
stagingBuffer.<span class="hljs-title function_">unmap</span>();
</code></pre>
<p>This is also a (bad) way to benchmark the time it takes for the instructions to be sent and the kernel to be launched. It's not optimal because it depends on CPU/GPU schedules, making it only somewhat reliable when the program is running constantly under a heavy load.</p>
<div id="section5"></div>
<h3>Benchmarking GPU functions</h3>
<p>This is taken from this <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-timing.html">website</a> I mentioned earlier, and it's a method to benchmark kernel executing time only, so although it may vary it doesn't depend on the CPU schedule and ability to synchronize. I modified the helper class to remove the asserts and convert the output to microseconds. The class:</p>
<details><summary>TimingHelper class</summary>
<pre><code class="language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TimingHelper</span> {
    #canTimestamp;
    #device;
    #querySet;
    #resolveBuffer;
    #resultBuffer;
    #resultBuffers = [];
    <span class="hljs-comment">// state can be &#x27;free&#x27;, &#x27;need resolve&#x27;, &#x27;wait for result&#x27;</span>
    #state = <span class="hljs-string">&#x27;free&#x27;</span>;

    <span class="hljs-title function_">constructor</span>(<span class="hljs-params">device</span>) {
    <span class="hljs-variable language_">this</span>.#device = device;
    <span class="hljs-variable language_">this</span>.#canTimestamp = device.<span class="hljs-property">features</span>.<span class="hljs-title function_">has</span>(<span class="hljs-string">&#x27;timestamp-query&#x27;</span>);
    <span class="hljs-keyword">if</span> (<span class="hljs-variable language_">this</span>.#canTimestamp) {
        <span class="hljs-variable language_">this</span>.#querySet = device.<span class="hljs-title function_">createQuerySet</span>({
        <span class="hljs-attr">type</span>: <span class="hljs-string">&#x27;timestamp&#x27;</span>,
        <span class="hljs-attr">count</span>: <span class="hljs-number">2</span>,
        });
        <span class="hljs-variable language_">this</span>.#resolveBuffer = device.<span class="hljs-title function_">createBuffer</span>({
        <span class="hljs-attr">size</span>: <span class="hljs-variable language_">this</span>.#querySet.<span class="hljs-property">count</span> * <span class="hljs-number">8</span>,
        <span class="hljs-attr">usage</span>: <span class="hljs-title class_">GPUBufferUsage</span>.<span class="hljs-property">QUERY_RESOLVE</span> | <span class="hljs-title class_">GPUBufferUsage</span>.<span class="hljs-property">COPY_SRC</span>,
        });
    }
    }
    
    #<span class="hljs-title function_">beginTimestampPass</span>(<span class="hljs-params">encoder, fnName, descriptor</span>) {
    <span class="hljs-keyword">if</span> (<span class="hljs-variable language_">this</span>.#canTimestamp) {
        <span class="hljs-variable language_">this</span>.#state = <span class="hljs-string">&#x27;need resolve&#x27;</span>;
    
        <span class="hljs-keyword">const</span> pass = encoder[fnName]({
        ...descriptor,
        ...{
            <span class="hljs-attr">timestampWrites</span>: {
            <span class="hljs-attr">querySet</span>: <span class="hljs-variable language_">this</span>.#querySet,
            <span class="hljs-attr">beginningOfPassWriteIndex</span>: <span class="hljs-number">0</span>,
            <span class="hljs-attr">endOfPassWriteIndex</span>: <span class="hljs-number">1</span>,
            },
        },
        });
    
        <span class="hljs-keyword">const</span> <span class="hljs-title function_">resolve</span> = (<span class="hljs-params"></span>) =&gt; <span class="hljs-variable language_">this</span>.#<span class="hljs-title function_">resolveTiming</span>(encoder);
        pass.<span class="hljs-property">end</span> = (<span class="hljs-keyword">function</span>(<span class="hljs-params">origFn</span>) {
        <span class="hljs-keyword">return</span> <span class="hljs-keyword">function</span>(<span class="hljs-params"></span>) {
            origFn.<span class="hljs-title function_">call</span>(<span class="hljs-variable language_">this</span>);
            <span class="hljs-title function_">resolve</span>();
        };
        })(pass.<span class="hljs-property">end</span>);
    
        <span class="hljs-keyword">return</span> pass;
    } <span class="hljs-keyword">else</span> {
        <span class="hljs-keyword">return</span> encoder[fnName](descriptor);
    }
    }
    
    <span class="hljs-title function_">beginRenderPass</span>(<span class="hljs-params">encoder, descriptor = {}</span>) {
    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">this</span>.#<span class="hljs-title function_">beginTimestampPass</span>(encoder, <span class="hljs-string">&#x27;beginRenderPass&#x27;</span>, descriptor);
    }
    
    <span class="hljs-title function_">beginComputePass</span>(<span class="hljs-params">encoder, descriptor = {}</span>) {
    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">this</span>.#<span class="hljs-title function_">beginTimestampPass</span>(encoder, <span class="hljs-string">&#x27;beginComputePass&#x27;</span>, descriptor);
    }
    
    #<span class="hljs-title function_">resolveTiming</span>(<span class="hljs-params">encoder</span>) {
    <span class="hljs-keyword">if</span> (!<span class="hljs-variable language_">this</span>.#canTimestamp) {
        <span class="hljs-keyword">return</span>;
    }
    <span class="hljs-variable language_">this</span>.#state = <span class="hljs-string">&#x27;wait for result&#x27;</span>;
    
    <span class="hljs-variable language_">this</span>.#resultBuffer = <span class="hljs-variable language_">this</span>.#resultBuffers.<span class="hljs-title function_">pop</span>() || <span class="hljs-variable language_">this</span>.#device.<span class="hljs-title function_">createBuffer</span>({
        <span class="hljs-attr">size</span>: <span class="hljs-variable language_">this</span>.#resolveBuffer.<span class="hljs-property">size</span>,
        <span class="hljs-attr">usage</span>: <span class="hljs-title class_">GPUBufferUsage</span>.<span class="hljs-property">COPY_DST</span> | <span class="hljs-title class_">GPUBufferUsage</span>.<span class="hljs-property">MAP_READ</span>,
    });
    
    encoder.<span class="hljs-title function_">resolveQuerySet</span>(<span class="hljs-variable language_">this</span>.#querySet, <span class="hljs-number">0</span>, <span class="hljs-variable language_">this</span>.#querySet.<span class="hljs-property">count</span>, <span class="hljs-variable language_">this</span>.#resolveBuffer, <span class="hljs-number">0</span>);
    encoder.<span class="hljs-title function_">copyBufferToBuffer</span>(<span class="hljs-variable language_">this</span>.#resolveBuffer, <span class="hljs-number">0</span>, <span class="hljs-variable language_">this</span>.#resultBuffer, <span class="hljs-number">0</span>, <span class="hljs-variable language_">this</span>.#resultBuffer.<span class="hljs-property">size</span>);
    }
    
    <span class="hljs-keyword">async</span> <span class="hljs-title function_">getResult</span>(<span class="hljs-params"></span>) { <span class="hljs-comment">// Returns result in microseconds</span>
    <span class="hljs-keyword">if</span> (!<span class="hljs-variable language_">this</span>.#canTimestamp) {
        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;
    }
    <span class="hljs-variable language_">this</span>.#state = <span class="hljs-string">&#x27;free&#x27;</span>;
    
    <span class="hljs-keyword">const</span> resultBuffer = <span class="hljs-variable language_">this</span>.#resultBuffer;
    <span class="hljs-keyword">await</span> resultBuffer.<span class="hljs-title function_">mapAsync</span>(<span class="hljs-title class_">GPUMapMode</span>.<span class="hljs-property">READ</span>);
    <span class="hljs-keyword">const</span> times = <span class="hljs-keyword">new</span> <span class="hljs-title class_">BigInt64Array</span>(resultBuffer.<span class="hljs-title function_">getMappedRange</span>());
    <span class="hljs-keyword">const</span> duration = <span class="hljs-title class_">Number</span>((times[<span class="hljs-number">1</span>] - times[<span class="hljs-number">0</span>])/<span class="hljs-number">1000n</span>);
    resultBuffer.<span class="hljs-title function_">unmap</span>();
    <span class="hljs-variable language_">this</span>.#resultBuffers.<span class="hljs-title function_">push</span>(resultBuffer);
    <span class="hljs-keyword">return</span> duration;
    }
}
</code></pre>
</details>
<p>Get an instance and use it in place of the encoder:</p>
<pre><code class="language-js"><span class="hljs-keyword">const</span> timingHelper = <span class="hljs-keyword">new</span> <span class="hljs-title class_">TimingHelper</span>(device);
...
<span class="hljs-keyword">const</span> encoder = device.<span class="hljs-title function_">createCommandEncoder</span>();
...
<span class="hljs-keyword">const</span> computePass = timingHelper.<span class="hljs-title function_">beginComputePass</span>(encoder);
...
computePass.<span class="hljs-title function_">end</span>();
...
device.<span class="hljs-property">queue</span>.<span class="hljs-title function_">submit</span>([encoder.<span class="hljs-title function_">finish</span>()]);
<span class="hljs-keyword">const</span> gpuTime = timingHelper.<span class="hljs-title function_">getResult</span>(); <span class="hljs-comment">// Microseconds</span>
</code></pre>
<div id="section6"></div>
<h3>Common problems</h3>
<ul>
<li><strong>Memory race conditions</strong>. Happens when memory is accessed at the same time by multiple threads. For example, if a thread needs to add a plus 1 to a position in global or shared memory this could raise a race condition:</li>
</ul>
<pre><code class="language-vhdl">  globalarray[i] += <span class="hljs-number">1</span>;
</code></pre>
<p>Under the hood, the thread will create a local copy of the value, add plus one and then deposit it. It's not instantaneous so you can understand how problematic it can be when multiple threads are tasked to do the same thing. In shared memory there is a function to synchronize threads and we can use it to deal with it. For global memory there are atomic operations.</p>
<ul>
<li><strong>Unsigned integer underflows</strong>. When one has to look in the previous position in an array, checking with unsigned integers can cause problems if handled incorrectly:</li>
</ul>
<pre><code class="language-rust">  <span class="hljs-keyword">if</span> (cellX - <span class="hljs-number">1</span> &gt;= <span class="hljs-number">0</span>) <span class="hljs-comment">// Wrong. If cellX == 0u, cellX - 1 will underflow into the max UINT32, which is &gt; 0</span>
  <span class="hljs-keyword">if</span> (cellX &gt; <span class="hljs-number">0</span>) <span class="hljs-comment">// Correct way.</span>
</code></pre>
<ul>
<li><strong>Shared bank memory conflicts.</strong> Available shared memory is divided into 32 bit banks. A conflict arises when two threads in the same warp attempt to access the same bank simultaneously, for which access is then serialized, slowing things down the more threads get in queue. An exception is when all threads in a warp attempt to do so; there is no conflict because data is broadcasted.</li>
</ul>
<div id="section7"></div>
<h3>Optimization techniques</h3>
<p>GPU scheduling:</p>
<ul>
<li><strong>Minimize GPU-CPU memory transfers.</strong> Unless it's some small value at every frame or some 1-time transfer, it's too expensive.</li>
<li><strong>Use bind groups smartly.</strong> They affect the way things are cached in GPU memory, therefore you want to group similar purpose buffers with each other. I.e. buffers that are copied frequently into the CPU (each frame) vs. buffers that are uploaded just once.</li>
<li><strong>Batch dispatches.</strong> Multiple compute shaders in the same pass and batching render and compute passes together before submitting to the queue once.</li>
<li><strong>Consider warp size.</strong> It's recommended to choose workgroup sizes that are multiple of the warp size, which in the case of NVIDIA GPUs it's 32.</li>
</ul>
<p>GPU instructions:</p>
<ul>
<li>
<p><strong>Optimize memory transfers.</strong> Data transfers in a GPU are often more expensive than the computation done in each thread, it's therefore fundamental to manage it adequately. According to CUDA (not saying WebGPU exposes all of these) there are a number of different types of memory:</p>
<ul>
<li><strong>Global memory</strong>. The largest, but also the slowest, can be accessed by all threads. Optimized for linear access.</li>
<li><strong>Shared memory</strong>. Small, fast, accessed by threads within a workgroup. Subject to bank memory conflicts, without them it can be just as fast as registers. Try these do not happen.</li>
<li><strong>Registers</strong>. Small, fastest, access is restricted per thread. When registers are not enough, the thread will pull from <em><strong>local memory</strong></em>, which resides cached in global memory and can be up to 150x slower.</li>
<li><strong>Constant memory</strong>. Cached read-only living in global memory. Can be faster than global memory if the access pattern is predictable and roughly one address per workgroup or warp. It's small, listed at 64 kB for some architectures.</li>
<li><strong>Texture memory</strong>. Cached read-only living in global memory. Optimized for small 2D data clusters, with special texture samplers that provide free bilinear filtering and other features.</li>
</ul>
</li>
<li>
<p><strong>Encourage memory coalescing.</strong> The more randomly accessed memory is, the worst. Encourage patterns where access is somewhat spatially continuous.</p>
</li>
<li>
<p><strong>Avoid branching inside workgroups.</strong> Conditionals may introduce branching, that is, stopping a thread's operation while the rest of the workgroup is active. It's often unavoidable, but it reduces GPU's occupancy and efficiency.</p>
</li>
<li>
<p><strong>Bit-packing.</strong> This can save on memory transfers in return for some computational cost, especially since GPUs only work with 32 bit numbers. Easy to explain with colors i.e. an RGBA quad of uint8s into an uint32, but this can be done with other variables.</p>
</li>
<li>
<p><strong>Use <abbr title="Fused multiply-add">FMA</abbr> (a,b,c) in place of floating point a*b+c.</strong> More precise and takes 1 instruction instead of two.</p>
</li>
<li>
<p><strong>Precompute divisors.</strong> Since divisions are more expensive than multiplication, precompute as divisors as constants like <code>INV_255: f32 = 1./255.</code></p>
</li>
</ul>
<p>I think these become second-hand as you learn WebGPU and deal with its challenges.</p>
<div id="section8"></div>
<h2>Algorithms</h2>
<div id="section9"></div>
<h3>Parallel reduction</h3>
<p>This is loosely based on NVIDIA's <a href="https://developer.download.nvidia.com/../../content/blog/assets/cuda/files/reduction.pdf">webinar</a> on parallel reduction. As a note, GPUs have changed quite a bit since then, and apparently there are slightly better methods now, but I'll stick to the basics. The task is to sum the integers in an array. I've also included these single-threaded CPU &amp; GPU versions for comparison:</p>
<pre><code class="language-js"><span class="hljs-comment">// CPU ST</span>
<span class="hljs-keyword">const</span> sum = numberArray.<span class="hljs-title function_">reduce</span>(<span class="hljs-function">(<span class="hljs-params">accumulator, currentValue</span>) =&gt;</span> accumulator + currentValue, <span class="hljs-number">0</span>);
</code></pre>
<p>For the rest of the functions I measured the time using the method mentioned earlier in the benchmarking subsection (so I'm not counting kernel launch delay and data transfers):</p>
<pre><code class="language-rust"><span class="hljs-comment">// Number of workgroups = 1</span>
<span class="hljs-comment">// GPU ST</span>
@compute @<span class="hljs-title function_ invoke__">workgroup_size</span>(<span class="hljs-number">1</span>)
<span class="hljs-keyword">fn</span> <span class="hljs-title function_">sumST</span>() {
    <span class="hljs-keyword">let</span> <span class="hljs-variable">nNumbers</span> = size;

    var sum = <span class="hljs-number">0</span>u;
    <span class="hljs-keyword">for</span> (var i = <span class="hljs-number">0</span>u; i &lt; nNumbers; i++) {
        sum += inputGlobal[i];
    }
    outputGlobal[<span class="hljs-number">0</span>] = sum;
}
</code></pre>
<p>Now let's do a parallel reduction. The method basically consists in treating the workgroup as a tree where data is summed up from the leaves (all threads) to the root (thread 0) in parallel. In the first few methods described here, each thread 0 of every workgroup deposits its sum on a smaller auxiliary array, which means multiple kernel launches are necessary if <code>data.length &lt; WORKGROUP_SIZE</code>.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Y1wOMUBsnJt9dV9iHUYhyQ.png" alt="Parallel reduction scheme (V0). @Source@NVIDIA"></p>
<p>Parallel reduction relies on shared memory by first having each thread copying in parallel from global memory, which greatly speeds up things afterwards as access to shared memory are very fast. To ensure race-conditions don't happen, we also have to raise workgroup-level synchronization barriers.</p>
<pre><code class="language-rust"><span class="hljs-comment">// Number of workgroups = ceil(data.length / WORKGROUP_SIZE)</span>
<span class="hljs-comment">// V0 - interleaved addressing</span>
var&lt;workgroup&gt; sdata: array&lt;<span class="hljs-type">u32</span>, WORKGROUP_SIZE&gt;;
@compute @<span class="hljs-title function_ invoke__">workgroup_size</span>(WORKGROUP_SIZE)
<span class="hljs-keyword">fn</span> <span class="hljs-title function_">sumReduce0</span>(
    @<span class="hljs-title function_ invoke__">builtin</span>(global_invocation_id) global_invocation_id: vec3u,
    @<span class="hljs-title function_ invoke__">builtin</span>(local_invocation_id) local_invocation_id: vec3u,
    @<span class="hljs-title function_ invoke__">builtin</span>(workgroup_id) workgroup_id: vec3u
) {
    <span class="hljs-keyword">let</span> <span class="hljs-variable">globalID</span> = global_invocation_id.x;
    <span class="hljs-keyword">let</span> <span class="hljs-variable">localID</span> = local_invocation_id.x;
    <span class="hljs-keyword">let</span> <span class="hljs-variable">workgroupID</span> = workgroup_id.x;

    sdata[localID] = inputGlobal[globalID];
    <span class="hljs-title function_ invoke__">workgroupBarrier</span>();

    <span class="hljs-keyword">for</span> (var s = <span class="hljs-number">1</span>u; s &lt; WORKGROUP_SIZE; s *= <span class="hljs-number">2</span>) {
        <span class="hljs-keyword">if</span> (localID % (<span class="hljs-number">2</span> * s) == <span class="hljs-number">0</span>) {
            sdata[localID] += sdata[localID + s];
        }
        <span class="hljs-title function_ invoke__">workgroupBarrier</span>();
    }
    <span class="hljs-keyword">if</span> (localID == <span class="hljs-number">0</span>) {
        outputGlobal[workgroupID] = sdata[<span class="hljs-number">0</span>];
    }
}
</code></pre>
<p>Once copied into shared memory, each thread loops in powers of two and checks if its own ID is a multiple of twice of that. So in the first iteration s=1 and only evenly numbered threads work, in the 2<sup>nd</sup> one s=2 and only threads 0, 4, 8 work, then 0, 8, 16, then 0, 16, 32... and so on. Each thread copies the value in memory i+1, i+2, i+4, etc. The process continues until the sum is collected at 0. This is much faster the single-threaded alternatives for any decently sized array. But it has problems:</p>
<ul>
<li>Each workgroup processes <strong>1 data point per thread</strong>, so the first kernel will launch <code>ceil(data.length/WORKGROUP_SIZE)</code>. There is a max number of dispatches per kernel, so after a certain size you will have to change that option if possible, increase workgroup size... Apart from this, if the computation is light, the optimal number of points each thread should be handling is &gt; 1. Half of the threads also do not nothing after retrieving the data from global memory.</li>
<li>The modulo <strong>operator % is expensive</strong>.</li>
<li>Highly <strong>divergent warps</strong>. On first iteration, the threads that work are 0, 2, 4... Threads come bunched up in groups called warps, and we should aim for them to have the most similar execution paths. If warps were of size 4 and our workgroup is size 8, it's considerably better if the threads that work are 0-1-2-3 instead of 0-2-4-6</li>
<li><strong>Sparse memory accesses</strong>. The average distance between memory accesses in each iteration is roughly the same, we are not taking advantage of locality.</li>
</ul>
<p>Let's change the inner loop in V2:</p>
<p><img src="https://raw.githubusercontent.com/mateuszbuda/GPUExample/master/reduce2.png" alt="V1. Source@NVIDIA"></p>
<pre><code class="language-rust"><span class="hljs-comment">// V1 - Interleaved addressing with strided index</span>
...
		<span class="hljs-keyword">let</span> <span class="hljs-variable">index</span> = <span class="hljs-number">2</span>*s*localID;
		<span class="hljs-keyword">if</span> (index &lt; WORKGROUP_SIZE) {
         	sdata[index] += sdata[index + s];
 		}
...
</code></pre>
<p>Now we built the index directly, removing the costly % operator and indexing (in the first iteration) with the first half of threads, so removing the divergence. This however introduced new issues:</p>
<ul>
<li>
<p><strong>Shared memory bank conflicts</strong>. As explained in the common problems subsection, these conflicts arise when multiple threads of the same warp access the same memory bank, forcing serialization and slowing things down. Let's imagine a case such as the one in the images shown, 16 data points, 8 threads per warp and 8 memory banks. In the first iteration:</p>
<ul>
<li>V0: Threads of two warps (0, 2, 4, 6 and 8, 10, 12, 14) access the 8 memory banks, no conflict.</li>
<li>V1: Threads of a single warp (0, 1, 2, 3, 4, 5, 6, 7) access the same banks, but for example now bank 0 and 1 are accessed by threads 0 and 4, which belong to the same warp and produces serialization.</li>
</ul>
</li>
<li>
<p><strong>Cache issues</strong>. In V0, threads always summed up the same location in memory plus a different one. This facilitated caching, but now only thread 0 does it.</p>
</li>
</ul>
<p>Version 3, sequential addressing:</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Slpu0FWHir7RIMMcAqN1xg.png" alt="V2. Source@NVIDIA"></p>
<pre><code class="language-rust"><span class="hljs-comment">// V2 - Sequential addressing</span>
...
<span class="hljs-keyword">for</span> (var s = WORKGROUP_SIZE/<span class="hljs-number">2</span>; s &gt; <span class="hljs-number">0</span>; s &gt;&gt;= <span class="hljs-number">1</span>) {
    <span class="hljs-keyword">if</span> (localID &lt; s) {
        sdata[localID] += sdata[localID + s];
    }
    <span class="hljs-title function_ invoke__">workgroupBarrier</span>();
}
...
</code></pre>
<p>Notice how both issues are fixed. With respect to the cache, thread i is always summing up whatever is on bank i plus something else. Regarding the memory conflicts, in the first iteration bank i is accessed twice by the same thread, and from there on there are no further problems.</p>
<p>This also fixed the sparse access pattern by utilizing a coalesced access approach, so there's left only one issue left, doing more work per thread. V3 deals with this to some extent, loading <em>two</em> values instead. Consequently, the number of workgroups need to be halved.</p>
<pre><code class="language-rust"><span class="hljs-comment">// Number of workgroups = ceil(data.length / WORKGROUP_SIZE / 2)</span>
<span class="hljs-comment">// V3 - First add during load</span>
...
<span class="hljs-keyword">let</span> <span class="hljs-variable">globalID</span> = workgroupID * WORKGROUP_SIZE*<span class="hljs-number">2</span> + localID;
sdata[localID] = inputGlobal[globalID] + inputGlobal[globalID + WORKGROUP_SIZE];
...
</code></pre>
<p>Workgroup sizes should be chosen (to help with performance) on multiples of 32 and equal or larger than 64 threads, due to the size of warps. This means we can unroll our loop, saving on instructions. As opposed to the original webinar where Mark doesn't add if checks for thread IDs &lt; 32 because of SIMD operations, we are going to have to keep them because WebGPU as far as I'm aware doesn't expose volatile variables yet. It's a reserved keyword and may be implemented in the future but I imagine it's an issue if the feature isn't present across every GPU architecture. V4 only unrolled &lt; 32 so let's skip it:</p>
<pre><code class="language-rust"><span class="hljs-comment">// V4/5 - Completely unrolled loop</span>
...
<span class="hljs-keyword">if</span> (WORKGROUP_SIZE &gt;= <span class="hljs-number">2048</span>) { <span class="hljs-keyword">if</span> (localID &lt; <span class="hljs-number">1024</span>) { sdata[localID] += sdata[localID + <span class="hljs-number">1024</span>]; } <span class="hljs-title function_ invoke__">workgroupBarrier</span>();}
<span class="hljs-keyword">if</span> (WORKGROUP_SIZE &gt;= <span class="hljs-number">1024</span>) { <span class="hljs-keyword">if</span> (localID &lt; <span class="hljs-number">512</span>) { sdata[localID] += sdata[localID + <span class="hljs-number">512</span>]; } <span class="hljs-title function_ invoke__">workgroupBarrier</span>();}
<span class="hljs-keyword">if</span> (WORKGROUP_SIZE &gt;= <span class="hljs-number">512</span>) { <span class="hljs-keyword">if</span> (localID &lt; <span class="hljs-number">256</span>) { sdata[localID] += sdata[localID + <span class="hljs-number">256</span>]; } <span class="hljs-title function_ invoke__">workgroupBarrier</span>();}
<span class="hljs-keyword">if</span> (WORKGROUP_SIZE &gt;= <span class="hljs-number">256</span>) { <span class="hljs-keyword">if</span> (localID &lt; <span class="hljs-number">128</span>) { sdata[localID] += sdata[localID + <span class="hljs-number">128</span>]; } <span class="hljs-title function_ invoke__">workgroupBarrier</span>();}
<span class="hljs-keyword">if</span> (WORKGROUP_SIZE &gt;= <span class="hljs-number">128</span>) { <span class="hljs-keyword">if</span> (localID &lt; <span class="hljs-number">64</span>) { sdata[localID] += sdata[localID + <span class="hljs-number">64</span>]; } <span class="hljs-title function_ invoke__">workgroupBarrier</span>();}
<span class="hljs-keyword">if</span> (localID &lt; <span class="hljs-number">32</span>) { sdata[localID] += sdata[localID + <span class="hljs-number">32</span>]; } <span class="hljs-title function_ invoke__">workgroupBarrier</span>();
<span class="hljs-keyword">if</span> (localID &lt; <span class="hljs-number">16</span>) { sdata[localID] += sdata[localID + <span class="hljs-number">16</span>]; } <span class="hljs-title function_ invoke__">workgroupBarrier</span>();
<span class="hljs-keyword">if</span> (localID &lt; <span class="hljs-number">8</span>) { sdata[localID] += sdata[localID + <span class="hljs-number">8</span>]; } <span class="hljs-title function_ invoke__">workgroupBarrier</span>();
<span class="hljs-keyword">if</span> (localID &lt; <span class="hljs-number">4</span>) { sdata[localID] += sdata[localID + <span class="hljs-number">4</span>]; } <span class="hljs-title function_ invoke__">workgroupBarrier</span>();
<span class="hljs-keyword">if</span> (localID &lt; <span class="hljs-number">2</span>) { sdata[localID] += sdata[localID + <span class="hljs-number">2</span>]; } <span class="hljs-title function_ invoke__">workgroupBarrier</span>();
<span class="hljs-keyword">if</span> (localID &lt; <span class="hljs-number">1</span>) { sdata[localID] += sdata[localID + <span class="hljs-number">1</span>]; } <span class="hljs-title function_ invoke__">workgroupBarrier</span>();
...
</code></pre>
<p>Mark then analyzes algorithm complexity and cites Brent's theorem that says each thread should sum o(logN) elements, due to the intrinsic cost of combining parallel and sequential processing. He goes even further, explaining there are other motives to increase elements per thread:</p>
<ul>
<li>Less kernel launch overhead because of fewer less levels needed.</li>
<li>Gains due to latency hiding because of heavier work per thread.</li>
</ul>
<p>I've also thought of a few:</p>
<ul>
<li>Less writes into auxiliary global memory arrays in between kernels, and these are smaller too.</li>
<li>If using somewhat expensive atomic operations to achieve the result in one kernel, the more work a thread does, the fewer atomic operations are needed.</li>
</ul>
<p>Let's briefly explain the concept of <em>latency-hiding</em>. Memory load and instruction latency are the most common latencies, and for GPUs they usually are in that order of importance too. Threads typically stall while waiting for these, and the GPU microprocessor switches between them to achieve much higher throughputs. This is the reason many threads should be ran so those latencies play a smaller role. However, the reason why higher work per thread improves this still escapes my understanding.</p>
<pre><code class="language-rust"><span class="hljs-comment">// Number of workgroups = ceil(data.length / WORKGROUP_SIZE / COARSE_FACTOR)</span>
<span class="hljs-comment">// V6.1 - Multiple elements per thread</span>
...
<span class="hljs-keyword">let</span> <span class="hljs-variable">globalID</span> = workgroupID * WORKGROUP_SIZE*<span class="hljs-number">2</span> + localID;
sdata[localID] = inputGlobal[globalID] + inputGlobal[globalID + WORKGROUP_SIZE];
var globalID = workgroupID * WORKGROUP_SIZE * COARSE_FACTOR + localID;

var sum = <span class="hljs-number">0</span>u;
<span class="hljs-keyword">for</span> (var tile = <span class="hljs-number">0</span>u; tile &lt; COARSE_FACTOR; tile++) {
    <span class="hljs-keyword">if</span> (globalID &lt; size) {
        sum +=  inputGlobal[globalID];
    }
    globalID += WORKGROUP_SIZE;
}
sdata[localID] = sum;
...
</code></pre>
<p>V6 is different than what Mark wrote because at first I did not understand its kernel launch and accessing pattern. I decided to leave it in because it's easier to understand, albeit I'm sure not as efficient. <strong>Work in progress after this</strong></p>
<pre><code class="language-rust"><span class="hljs-comment">// Number of workgroups = </span>
<span class="hljs-comment">// V6.2 - Multiple elements per thread</span>
</code></pre>
<h4>Min, max, argmin and argmax variants</h4>
<p>I'm leaving also some other useful computations using parallel reduction (only the parts that change):</p>
<pre><code class="language-rust"><span class="hljs-comment">// Minimum</span>
...
var minValue = MAX_UINT32;
...
	<span class="hljs-keyword">if</span> (globalID &lt; size) {
    	<span class="hljs-keyword">let</span> <span class="hljs-variable">value</span> = inputGlobal[globalID];
        <span class="hljs-keyword">if</span> (value &lt; minValue) {
            minValue = value;
        }
	}
...
<span class="hljs-comment">// The operation done in each part of the unrolled loop, with id1 and id2 the first and second elements</span>

</code></pre>
<div id="section10"></div>
<h3>PseudoRandom Number generator</h3>
<p>PCG hash <abbr title="Pseudorandom number">PRN</abbr> generator:</p>
<pre><code class="language-rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">randomU32</span>(seed: <span class="hljs-type">u32</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">u32</span> { <span class="hljs-comment">// PCG hash random int generator</span>
    <span class="hljs-keyword">let</span> <span class="hljs-variable">state</span> = seed * <span class="hljs-number">747796405</span>u + <span class="hljs-number">2891336453</span>u;
    <span class="hljs-keyword">let</span> <span class="hljs-variable">word</span> = ((state &gt;&gt; ((state &gt;&gt; <span class="hljs-number">28</span>u) + <span class="hljs-number">4</span>u)) ^ state) * <span class="hljs-number">277803737</span>u;
    <span class="hljs-title function_ invoke__">return</span> (word &gt;&gt; <span class="hljs-number">22</span>u) ^ word;
}

<span class="hljs-keyword">fn</span> <span class="hljs-title function_">randomF32</span>(seed: <span class="hljs-type">u32</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">f32</span> { <span class="hljs-comment">// PCG hash random float generator</span>
    <span class="hljs-keyword">let</span> <span class="hljs-variable">MAX_UINT32</span> = <span class="hljs-number">4294967295</span>u;
    <span class="hljs-keyword">return</span> <span class="hljs-title function_ invoke__">f32</span>(<span class="hljs-title function_ invoke__">randomU32</span>(seed))/<span class="hljs-title function_ invoke__">f32</span>(MAX_UINT32);
}
</code></pre>
<hr><div class="blogTags"><button class="blogTagSelected" title='Check more posts of the "Reference" category on my blog!'>Reference</button></div><br></div>
        <div style="flex: 1; min-width: 20px;"></div>
    </div>
</div>
<script>
// Table of contents fix for correct opening and closing of detail-summary blocks
function toggleDetails(event) {
    if (event.offsetX < 0) {
        event.stopPropagation();
        const details = event.currentTarget.firstChild;
        details.open = !details.open;
    } else if (event.offsetY < 30) {
        event.stopPropagation();
        const details = event.currentTarget.firstChild;
        details.open = true;
    }
}
window.onload = function() {   
    // Add figure number and caption to all images
    const images = document.querySelectorAll("#HTMLpost img");
    let figureNumber = 0;
    images.forEach(img => {
        if (img.alt === "INLINE") { return; }

        figureNumber++;

        img.style.display = "block";
        img.style.margin = "auto";

        const caption = document.createElement("p");
        caption.textContent = `Figure ${figureNumber}. ${img.alt}`;
        caption.style.textAlign = "center";
        caption.style.fontSize = "0.9em"
        caption.style.marginTop = "0.5em";
        img.parentNode.insertBefore(caption, img.nextSibling);        
    });

    // Turn all details open by default in HTMLpost's TOC
    const details = document.querySelectorAll("#HTMLpost .TOCpost details");
    details.forEach(detail => detail.open = true);
};
</script>
</body>
</html>